{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c54c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "import gzip\n",
    "import json\n",
    "import io\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = boto3.Session().client(\"sts\", region_name=\"us-east-1\")\n",
    "response = sts.assume_role(\n",
    "    RoleArn=\"arn:aws:iam::375084544312:role/mimesample_delegate\",\n",
    "    RoleSessionName=\"mimesamples-access\"\n",
    ")\n",
    "\n",
    "ACCESS_KEY = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "SECRET_KEY = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "SESSION_TOKEN = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    aws_session_token=SESSION_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0aaadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "BUCKET = 'mime-samples-production'\n",
    "PREFIX = 'users'\n",
    "\n",
    "def list_json_files(user_id):\n",
    "    prefix = f\"{PREFIX}/{user_id}/\"\n",
    "    response = s3_client.list_objects_v2(Bucket=BUCKET, Prefix=prefix)\n",
    "    files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.json')]\n",
    "    return files\n",
    "\n",
    "def download_and_flatten(user_id):\n",
    "    files = list_json_files(user_id)\n",
    "    if not files:\n",
    "        return None\n",
    "    file_key = files[0]\n",
    "    obj = s3_client.get_object(Bucket=BUCKET, Key=file_key)\n",
    "    content = obj['Body'].read().decode('utf-8')\n",
    "    data = json.loads(content)\n",
    "    flattened = pd.json_normalize(data)\n",
    "    flattened['user_id'] = user_id\n",
    "    del content, data\n",
    "    gc.collect()\n",
    "    return flattened\n",
    "\n",
    "def process_user_ids(user_ids, temp_dir, process_id):\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    batch_size = 5000\n",
    "    buffer = []\n",
    "    current_size = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    for user_id in tqdm(user_ids, desc=f\"Process {process_id}\"):\n",
    "        try:\n",
    "            df = download_and_flatten(user_id)\n",
    "            if df is not None:\n",
    "                buffer.append(df)\n",
    "                current_size += len(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {user_id}: {e}\")\n",
    "\n",
    "        if current_size >= batch_size:\n",
    "            full_df = pd.concat(buffer, ignore_index=True)\n",
    "            batch_df = full_df.iloc[:batch_size]\n",
    "            batch_file = os.path.join(temp_dir, f\"partial_{process_id}_{batch_idx}.parquet\")\n",
    "            batch_df.to_parquet(batch_file, index=False)\n",
    "            print(f\"Process {process_id}: Wrote partial batch {batch_idx} with {len(batch_df)} records.\")\n",
    "            batch_idx += 1\n",
    "\n",
    "            remaining_df = full_df.iloc[batch_size:]\n",
    "            buffer = [remaining_df] if not remaining_df.empty else []\n",
    "            current_size = len(remaining_df)\n",
    "            del full_df, batch_df, remaining_df\n",
    "            gc.collect()\n",
    "\n",
    "    if buffer:\n",
    "        full_df = pd.concat(buffer, ignore_index=True)\n",
    "        batch_file = os.path.join(temp_dir, f\"partial_{process_id}_{batch_idx}.parquet\")\n",
    "        full_df.to_parquet(batch_file, index=False)\n",
    "        print(f\"Process {process_id}: Wrote final partial batch {batch_idx} with {len(full_df)} records.\")\n",
    "        del full_df, buffer\n",
    "        gc.collect()\n",
    "\n",
    "def merge_and_batch(temp_dir, final_output_dir, batch_size=5000):\n",
    "    os.makedirs(final_output_dir, exist_ok=True)\n",
    "    buffer = []\n",
    "    current_size = 0\n",
    "    batch_idx = 0\n",
    "    partial_files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith('.parquet')]\n",
    "\n",
    "    for file in partial_files:\n",
    "        df = pd.read_parquet(file)\n",
    "        buffer.append(df)\n",
    "        current_size += len(df)\n",
    "\n",
    "        while current_size >= batch_size:\n",
    "            full_buffer_df = pd.concat(buffer, ignore_index=True)\n",
    "            batch_df = full_buffer_df.iloc[:batch_size]\n",
    "            batch_file = os.path.join(final_output_dir, f\"batch_{batch_idx}.parquet\")\n",
    "            batch_df.to_parquet(batch_file, index=False)\n",
    "            print(f\"Wrote batch {batch_idx} with {len(batch_df)} records.\")\n",
    "            batch_idx += 1\n",
    "            remaining_df = full_buffer_df.iloc[batch_size:]\n",
    "            buffer = [remaining_df] if not remaining_df.empty else []\n",
    "            current_size = len(remaining_df)\n",
    "            del full_buffer_df, batch_df, remaining_df\n",
    "            gc.collect()\n",
    "\n",
    "    if buffer:\n",
    "        full_buffer_df = pd.concat(buffer, ignore_index=True)\n",
    "        if not full_buffer_df.empty:\n",
    "            batch_file = os.path.join(final_output_dir, f\"batch_{batch_idx}.parquet\")\n",
    "            full_buffer_df.to_parquet(batch_file, index=False)\n",
    "            print(f\"Wrote final batch {batch_idx} with {len(full_buffer_df)} records.\")\n",
    "            del full_buffer_df\n",
    "            gc.collect()\n",
    "\n",
    "def main():\n",
    "    with open('pending_list.txt', 'r') as f:\n",
    "        user_ids = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    n_jobs = min(32, len(user_ids))\n",
    "    chunk_size = len(user_ids) // n_jobs\n",
    "    chunks = [user_ids[i*chunk_size : (i+1)*chunk_size] for i in range(n_jobs)]\n",
    "\n",
    "    if len(user_ids) % n_jobs != 0:\n",
    "        chunks[-1].extend(user_ids[n_jobs*chunk_size:])\n",
    "\n",
    "    temp_dir = 'temp_output'\n",
    "    final_dir = 'final_output'\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        futures = []\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            futures.append(executor.submit(process_user_ids, chunk, temp_dir, idx))\n",
    "        for f in futures:\n",
    "            f.result()\n",
    "\n",
    "    merge_and_batch(temp_dir, final_dir, batch_size=5000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    end_time = time.time()\n",
    "    print(\"Time taken:\", end_time - start_time)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
